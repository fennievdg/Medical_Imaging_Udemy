{"cells":[{"cell_type":"markdown","id":"desirable-healthcare","metadata":{"id":"desirable-healthcare"},"source":["## Visualize the decision of the classifier\n","In this notebook we will learn how to use Class Acvitation Maps (CAM) (https://arxiv.org/abs/1512.04150). <br />\n","A method to visualize image regions most important for the decision of the classifier."]},{"cell_type":"markdown","id":"opposite-bundle","metadata":{"id":"opposite-bundle"},"source":["## Imports\n","\n","* torch for tensor manipulation\n","* torchvision for resnet18\n","* transforms for Normalization\n","* pytorch lightning for model creation\n","* numpy for data loading\n","* matplotlib for plotting"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oa_VSc67tXPL","executionInfo":{"status":"ok","timestamp":1656192114524,"user_tz":-120,"elapsed":36625,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"68a6df7b-220d-426d-e51c-115c0ba8247c"},"id":"oa_VSc67tXPL","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["root = \"/content/drive/My Drive/pytorch_udemy/04-Pneumonia-Classification/data/\""],"metadata":{"id":"NQoWrY9Vu0Y0","executionInfo":{"status":"ok","timestamp":1656192117961,"user_tz":-120,"elapsed":245,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}}},"id":"NQoWrY9Vu0Y0","execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install torchmetrics\n","!pip install pytorch_lightning "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDkYbSk9u5lO","executionInfo":{"status":"ok","timestamp":1656192134657,"user_tz":-120,"elapsed":15714,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"8fb54060-a6d4-4287-87bd-c821a251ad89"},"id":"JDkYbSk9u5lO","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.9.1-py3-none-any.whl (419 kB)\n","\u001b[K     |████████████████████████████████| 419 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-1.6.4-py3-none-any.whl (585 kB)\n","\u001b[K     |████████████████████████████████| 585 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.0)\n","Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (3.17.3)\n","Collecting pyDeprecate>=0.3.1\n","  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.8.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (0.9.1)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n","Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.11.0+cu113)\n","Collecting PyYAML>=5.4\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 46.2 MB/s \n","\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n","  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 57.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 40.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.46.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.7)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.1.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 33.4 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 56.9 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, PyYAML, pyDeprecate, pytorch-lightning\n","  Attempting uninstall: PyYAML\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-6.0 aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.5.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.4 yarl-1.7.2\n"]}]},{"cell_type":"code","execution_count":5,"id":"sealed-retreat","metadata":{"id":"sealed-retreat","executionInfo":{"status":"ok","timestamp":1656192155054,"user_tz":-120,"elapsed":516,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}}},"outputs":[],"source":["%matplotlib notebook\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import pytorch_lightning as pl\n","import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","id":"residential-stanley","metadata":{"id":"residential-stanley"},"source":["The dataset of the previous notebook is used."]},{"cell_type":"code","execution_count":6,"id":"latter-intake","metadata":{"id":"latter-intake","executionInfo":{"status":"ok","timestamp":1656192196647,"user_tz":-120,"elapsed":225,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}}},"outputs":[],"source":["def load_file(path):\n","    return np.load(path).astype(np.float32)\n"]},{"cell_type":"code","execution_count":8,"id":"painful-caribbean","metadata":{"id":"painful-caribbean","executionInfo":{"status":"ok","timestamp":1656192219982,"user_tz":-120,"elapsed":3063,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}}},"outputs":[],"source":["val_transforms = transforms.Compose([\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(0.49, 0.248),\n","\n","])\n","\n","val_dataset = torchvision.datasets.DatasetFolder(root + \"Processed/val/\", loader=load_file, extensions=\"npy\", transform=val_transforms)"]},{"cell_type":"markdown","id":"mature-density","metadata":{"id":"mature-density"},"source":["The key idea of CAM is to multiply the output of the last convolutional layer (BasicBlock 1 of layer 4) $A_k$ (consisting of k channels) with the parameters $w$ of the subsequent fully connected layer to compute an activation map $M$:\n","$$ M = \\sum_k w_kA_k$$\n","\n","To do so, we need to access this particular output of the trained resnet18.<br />\n","Let's recap the resnet architecture:"]},{"cell_type":"code","execution_count":9,"id":"clear-integration","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"clear-integration","executionInfo":{"status":"ok","timestamp":1656192693318,"user_tz":-120,"elapsed":604,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"bf6f5763-354d-4ffe-96de-dfcd9d74b5df"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",")"]},"metadata":{},"execution_count":9}],"source":["temp_model = torchvision.models.resnet18()\n","temp_model"]},{"cell_type":"markdown","id":"growing-calendar","metadata":{"id":"growing-calendar"},"source":["We can convert the network to a generator using the **children()** function.<br />\n","This means that we can use the list function to convert it into a list!<br />\n","The convolutional part of the network comprises all layers up to the AdaptiveAvgPool2d layer."]},{"cell_type":"code","execution_count":10,"id":"outer-trainer","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"outer-trainer","executionInfo":{"status":"ok","timestamp":1656192763683,"user_tz":-120,"elapsed":225,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"debe5fb2-69ea-4d69-fea6-2296805a5ce6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n"," BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n"," ReLU(inplace=True),\n"," MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," ),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (downsample): Sequential(\n","       (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","       (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     )\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," ),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (downsample): Sequential(\n","       (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     )\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," ),\n"," Sequential(\n","   (0): BasicBlock(\n","     (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (downsample): Sequential(\n","       (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","       (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     )\n","   )\n","   (1): BasicBlock(\n","     (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","     (relu): ReLU(inplace=True)\n","     (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","     (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","   )\n"," )]"]},"metadata":{},"execution_count":10}],"source":["list(temp_model.children())[:-2]  # get all layers up to avgpool"]},{"cell_type":"markdown","id":"immune-break","metadata":{"id":"immune-break"},"source":["Using **Sequential** from pytorch, we convert the list of layers back to a Sequential Model.\n"]},{"cell_type":"code","execution_count":11,"id":"through-commission","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"through-commission","executionInfo":{"status":"ok","timestamp":1656195826551,"user_tz":-120,"elapsed":252,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"de0c7ab4-c1fd-4aee-c74b-ebc7a91beca6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (2): ReLU(inplace=True)\n","  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (5): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (6): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (7): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":11}],"source":["torch.nn.Sequential(*list(temp_model.children())[:-2])"]},{"cell_type":"markdown","id":"narrative-transport","metadata":{"id":"narrative-transport"},"source":["Now we are ready to go.<br />\n","We add an additional output to the forward function of our pneumonia model, to return the feature maps of the last convolutional layer ($A$)\n","\n","We extract the feature map in the forward pass, followed by global average pooling and flattening.\n","Finally we use the fully connected layer to compute the final class prediction."]},{"cell_type":"code","execution_count":12,"id":"motivated-eagle","metadata":{"id":"motivated-eagle","executionInfo":{"status":"ok","timestamp":1656195834584,"user_tz":-120,"elapsed":221,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}}},"outputs":[],"source":["class PneumoniaModel(pl.LightningModule):\n","    def __init__(self):\n","        super().__init__()\n","        \n","        self.model = torchvision.models.resnet18()\n","        # Change conv1 from 3 to 1 input channels\n","        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","        # Change out_feature of the last fully connected layer (called fc in resnet18) from 1000 to 1\n","        self.model.fc = torch.nn.Linear(in_features=512, out_features=1)\n","        \n","        # Extract the feature map\n","        self.feature_map = torch.nn.Sequential(*list(self.model.children())[:-2])    \n","    def forward(self, data):\n","        \n","        # Compute feature map\n","        feature_map = self.feature_map(data)\n","        # Use Adaptive Average Pooling as in the original model\n","        avg_pool_output = torch.nn.functional.adaptive_avg_pool2d(input=feature_map, output_size=(1, 1))\n","        print(avg_pool_output.shape)\n","        # Flatten the output into a 512 element vector\n","        avg_pool_output_flattened = torch.flatten(avg_pool_output)\n","        print(avg_pool_output_flattened.shape)\n","        # Compute prediction\n","        pred = self.model.fc(avg_pool_output_flattened)\n","        return pred, feature_map\n","    \n"]},{"cell_type":"code","execution_count":null,"id":"alike-absence","metadata":{"id":"alike-absence"},"outputs":[],"source":["def cam(model, img):\n","    with torch.no_grad():\n","        pred, features = model(img.unsqueeze(0))\n","    features = features.reshape((512, 49))\n","    weight_params = list(model.model.fc.parameters())[0]\n","    weight = weight_params[0].detach()\n","    \n","    \n","    cam = torch.matmul(weight, features) \n","    cam_img = cam.reshape(7, 7).cpu()\n","    return cam_img, torch.sigmoid(pred)"]},{"cell_type":"code","execution_count":13,"id":"seventh-activity","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"seventh-activity","executionInfo":{"status":"ok","timestamp":1656195931409,"user_tz":-120,"elapsed":2309,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"ef1667a5-038c-4e9a-d598-289e8db47b20"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py:214: UserWarning: Found keys that are in the model state dict but not in the checkpoint: ['feature_map.0.weight', 'feature_map.1.weight', 'feature_map.1.bias', 'feature_map.1.running_mean', 'feature_map.1.running_var', 'feature_map.4.0.conv1.weight', 'feature_map.4.0.bn1.weight', 'feature_map.4.0.bn1.bias', 'feature_map.4.0.bn1.running_mean', 'feature_map.4.0.bn1.running_var', 'feature_map.4.0.conv2.weight', 'feature_map.4.0.bn2.weight', 'feature_map.4.0.bn2.bias', 'feature_map.4.0.bn2.running_mean', 'feature_map.4.0.bn2.running_var', 'feature_map.4.1.conv1.weight', 'feature_map.4.1.bn1.weight', 'feature_map.4.1.bn1.bias', 'feature_map.4.1.bn1.running_mean', 'feature_map.4.1.bn1.running_var', 'feature_map.4.1.conv2.weight', 'feature_map.4.1.bn2.weight', 'feature_map.4.1.bn2.bias', 'feature_map.4.1.bn2.running_mean', 'feature_map.4.1.bn2.running_var', 'feature_map.5.0.conv1.weight', 'feature_map.5.0.bn1.weight', 'feature_map.5.0.bn1.bias', 'feature_map.5.0.bn1.running_mean', 'feature_map.5.0.bn1.running_var', 'feature_map.5.0.conv2.weight', 'feature_map.5.0.bn2.weight', 'feature_map.5.0.bn2.bias', 'feature_map.5.0.bn2.running_mean', 'feature_map.5.0.bn2.running_var', 'feature_map.5.0.downsample.0.weight', 'feature_map.5.0.downsample.1.weight', 'feature_map.5.0.downsample.1.bias', 'feature_map.5.0.downsample.1.running_mean', 'feature_map.5.0.downsample.1.running_var', 'feature_map.5.1.conv1.weight', 'feature_map.5.1.bn1.weight', 'feature_map.5.1.bn1.bias', 'feature_map.5.1.bn1.running_mean', 'feature_map.5.1.bn1.running_var', 'feature_map.5.1.conv2.weight', 'feature_map.5.1.bn2.weight', 'feature_map.5.1.bn2.bias', 'feature_map.5.1.bn2.running_mean', 'feature_map.5.1.bn2.running_var', 'feature_map.6.0.conv1.weight', 'feature_map.6.0.bn1.weight', 'feature_map.6.0.bn1.bias', 'feature_map.6.0.bn1.running_mean', 'feature_map.6.0.bn1.running_var', 'feature_map.6.0.conv2.weight', 'feature_map.6.0.bn2.weight', 'feature_map.6.0.bn2.bias', 'feature_map.6.0.bn2.running_mean', 'feature_map.6.0.bn2.running_var', 'feature_map.6.0.downsample.0.weight', 'feature_map.6.0.downsample.1.weight', 'feature_map.6.0.downsample.1.bias', 'feature_map.6.0.downsample.1.running_mean', 'feature_map.6.0.downsample.1.running_var', 'feature_map.6.1.conv1.weight', 'feature_map.6.1.bn1.weight', 'feature_map.6.1.bn1.bias', 'feature_map.6.1.bn1.running_mean', 'feature_map.6.1.bn1.running_var', 'feature_map.6.1.conv2.weight', 'feature_map.6.1.bn2.weight', 'feature_map.6.1.bn2.bias', 'feature_map.6.1.bn2.running_mean', 'feature_map.6.1.bn2.running_var', 'feature_map.7.0.conv1.weight', 'feature_map.7.0.bn1.weight', 'feature_map.7.0.bn1.bias', 'feature_map.7.0.bn1.running_mean', 'feature_map.7.0.bn1.running_var', 'feature_map.7.0.conv2.weight', 'feature_map.7.0.bn2.weight', 'feature_map.7.0.bn2.bias', 'feature_map.7.0.bn2.running_mean', 'feature_map.7.0.bn2.running_var', 'feature_map.7.0.downsample.0.weight', 'feature_map.7.0.downsample.1.weight', 'feature_map.7.0.downsample.1.bias', 'feature_map.7.0.downsample.1.running_mean', 'feature_map.7.0.downsample.1.running_var', 'feature_map.7.1.conv1.weight', 'feature_map.7.1.bn1.weight', 'feature_map.7.1.bn1.bias', 'feature_map.7.1.bn1.running_mean', 'feature_map.7.1.bn1.running_var', 'feature_map.7.1.conv2.weight', 'feature_map.7.1.bn2.weight', 'feature_map.7.1.bn2.bias', 'feature_map.7.1.bn2.running_mean', 'feature_map.7.1.bn2.running_var']\n","  f\"Found keys that are in the model state dict but not in the checkpoint: {keys.missing_keys}\"\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/core/saving.py:218: UserWarning: Found keys that are not in the model state dict but in the checkpoint: ['loss_fn.pos_weight']\n","  f\"Found keys that are not in the model state dict but in the checkpoint: {keys.unexpected_keys}\"\n"]}],"source":["# Use strict to prevent pytorch from loading weights for self.feature_map, false so it loads the weights it can match. \n","model = PneumoniaModel.load_from_checkpoint(\"/content/drive/My Drive/pytorch_udemy/04-Pneumonia-Classification/weights/weights_3.ckpt\", strict=False)\n","model.eval();"]},{"cell_type":"markdown","id":"precise-discount","metadata":{"id":"precise-discount"},"source":["## CAM\n","Now we can define the CAM function by using the formula from above:"]},{"cell_type":"code","execution_count":14,"id":"stable-driving","metadata":{"id":"stable-driving","executionInfo":{"status":"ok","timestamp":1656195973247,"user_tz":-120,"elapsed":221,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}}},"outputs":[],"source":["def cam(model, img):\n","    \"\"\"\n","    Compute class activation map according to cam algorithm\n","    \"\"\"\n","    with torch.no_grad():\n","        pred, features = model(img.unsqueeze(0))\n","    b, c, h, w = features.shape\n","\n","    # We reshape the 512x7x7 feature tensor into a 512x49 tensor in order to simplify the multiplication\n","    features = features.reshape((c, h*w))\n","    \n","    # Get only the weights, not the bias\n","    weight_params = list(model.model.fc.parameters())[0] \n","    \n","    # Remove gradient information from weight parameters to enable numpy conversion\n","    weight = weight_params[0].detach()\n","    print(weight.shape)\n","    # Compute multiplication between weight and features with the formula from above.\n","    # We use matmul because it directly multiplies each filter with the weights\n","    # and then computes the sum. This yields a vector of 49 (7x7 elements)\n","    cam = torch.matmul(weight, features)\n","    print(features.shape)\n","    \n","    ### The following loop performs the same operations in a less optimized way\n","    #cam = torch.zeros((7 * 7))\n","    #for i in range(len(cam)):\n","    #    cam[i] = torch.sum(weight*features[:,i])\n","    ##################################################################\n","    \n","    # Normalize and standardize the class activation map (Not always necessary, thus not shown in the lecture)\n","    cam = cam - torch.min(cam)\n","    cam_img = cam / torch.max(cam)\n","    # Reshape the class activation map to 512x7x7 and move the tensor back to CPU\n","    cam_img = cam_img.reshape(h, w).cpu()\n","\n","    return cam_img, torch.sigmoid(pred)\n","\n","def visualize(img, heatmap, pred):\n","    \"\"\"\n","    Visualization function for class activation maps\n","    \"\"\"\n","    img = img[0]\n","    # Resize the activation map of size 7x7 to the original image size (224x224)\n","    heatmap = transforms.functional.resize(heatmap.unsqueeze(0), (img.shape[0], img.shape[1]))[0]\n","    \n","    # Create a figure\n","    fig, axis = plt.subplots(1, 2)\n","    \n","    axis[0].imshow(img, cmap=\"bone\")\n","    # Overlay the original image with the upscaled class activation map\n","    axis[1].imshow(img, cmap=\"bone\")\n","    axis[1].imshow(heatmap, alpha=0.5, cmap=\"jet\")\n","    plt.title(f\"Pneumonia: {(pred > 0.5).item()}\")"]},{"cell_type":"code","execution_count":27,"id":"fresh-casino","metadata":{"id":"fresh-casino","executionInfo":{"status":"ok","timestamp":1656196227428,"user_tz":-120,"elapsed":58,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}}},"outputs":[],"source":["def visualize(img, cam, pred):\n","    img = img[0]\n","    cam = transforms.functional.resize(cam.unsqueeze(0), (224, 224))[0]\n","    \n","    fig, axis = plt.subplots(1, 2)\n","    axis[0].imshow(img, cmap=\"bone\")\n","    axis[1].imshow(img, cmap=\"bone\")\n","    axis[1].imshow(cam, alpha=0.5, cmap=\"jet\")\n","    plt.title(pred)\n"]},{"cell_type":"markdown","id":"extended-departure","metadata":{"id":"extended-departure"},"source":["## Demo Time"]},{"cell_type":"code","execution_count":16,"id":"assigned-replacement","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"assigned-replacement","executionInfo":{"status":"ok","timestamp":1656195981395,"user_tz":-120,"elapsed":562,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"8100ecb5-8c0a-49a6-b367-3d619dee8163"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 512, 1, 1])\n","torch.Size([512])\n","torch.Size([512])\n","torch.Size([512, 49])\n"]}],"source":["img = val_dataset[-6][0]  # Select a subject\n","activation_map, pred = cam(model, img)  # Compute the Class activation map given the subject"]},{"cell_type":"code","source":["print(pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"3oYtaszt-YVm","executionInfo":{"status":"ok","timestamp":1656196361572,"user_tz":-120,"elapsed":254,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"3774b87e-abda-40e1-dcae-5496444552ad"},"id":"3oYtaszt-YVm","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0.5688])\n"]}]},{"cell_type":"code","execution_count":29,"id":"hungarian-lighting","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"hungarian-lighting","executionInfo":{"status":"ok","timestamp":1656196237410,"user_tz":-120,"elapsed":21,"user":{"displayName":"Fennie van der Graaf","userId":"13308186546935518868"}},"outputId":"1d3469bc-19b1-4393-b03a-87fc4b2c071f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["/* Put everything inside the global mpl namespace */\n","window.mpl = {};\n","\n","\n","mpl.get_websocket_type = function() {\n","    if (typeof(WebSocket) !== 'undefined') {\n","        return WebSocket;\n","    } else if (typeof(MozWebSocket) !== 'undefined') {\n","        return MozWebSocket;\n","    } else {\n","        alert('Your browser does not have WebSocket support. ' +\n","              'Please try Chrome, Safari or Firefox ≥ 6. ' +\n","              'Firefox 4 and 5 are also supported but you ' +\n","              'have to enable WebSockets in about:config.');\n","    };\n","}\n","\n","mpl.figure = function(figure_id, websocket, ondownload, parent_element) {\n","    this.id = figure_id;\n","\n","    this.ws = websocket;\n","\n","    this.supports_binary = (this.ws.binaryType != undefined);\n","\n","    if (!this.supports_binary) {\n","        var warnings = document.getElementById(\"mpl-warnings\");\n","        if (warnings) {\n","            warnings.style.display = 'block';\n","            warnings.textContent = (\n","                \"This browser does not support binary websocket messages. \" +\n","                    \"Performance may be slow.\");\n","        }\n","    }\n","\n","    this.imageObj = new Image();\n","\n","    this.context = undefined;\n","    this.message = undefined;\n","    this.canvas = undefined;\n","    this.rubberband_canvas = undefined;\n","    this.rubberband_context = undefined;\n","    this.format_dropdown = undefined;\n","\n","    this.image_mode = 'full';\n","\n","    this.root = $('<div/>');\n","    this._root_extra_style(this.root)\n","    this.root.attr('style', 'display: inline-block');\n","\n","    $(parent_element).append(this.root);\n","\n","    this._init_header(this);\n","    this._init_canvas(this);\n","    this._init_toolbar(this);\n","\n","    var fig = this;\n","\n","    this.waiting = false;\n","\n","    this.ws.onopen =  function () {\n","            fig.send_message(\"supports_binary\", {value: fig.supports_binary});\n","            fig.send_message(\"send_image_mode\", {});\n","            if (mpl.ratio != 1) {\n","                fig.send_message(\"set_dpi_ratio\", {'dpi_ratio': mpl.ratio});\n","            }\n","            fig.send_message(\"refresh\", {});\n","        }\n","\n","    this.imageObj.onload = function() {\n","            if (fig.image_mode == 'full') {\n","                // Full images could contain transparency (where diff images\n","                // almost always do), so we need to clear the canvas so that\n","                // there is no ghosting.\n","                fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n","            }\n","            fig.context.drawImage(fig.imageObj, 0, 0);\n","        };\n","\n","    this.imageObj.onunload = function() {\n","        fig.ws.close();\n","    }\n","\n","    this.ws.onmessage = this._make_on_message_function(this);\n","\n","    this.ondownload = ondownload;\n","}\n","\n","mpl.figure.prototype._init_header = function() {\n","    var titlebar = $(\n","        '<div class=\"ui-dialog-titlebar ui-widget-header ui-corner-all ' +\n","        'ui-helper-clearfix\"/>');\n","    var titletext = $(\n","        '<div class=\"ui-dialog-title\" style=\"width: 100%; ' +\n","        'text-align: center; padding: 3px;\"/>');\n","    titlebar.append(titletext)\n","    this.root.append(titlebar);\n","    this.header = titletext[0];\n","}\n","\n","\n","\n","mpl.figure.prototype._canvas_extra_style = function(canvas_div) {\n","\n","}\n","\n","\n","mpl.figure.prototype._root_extra_style = function(canvas_div) {\n","\n","}\n","\n","mpl.figure.prototype._init_canvas = function() {\n","    var fig = this;\n","\n","    var canvas_div = $('<div/>');\n","\n","    canvas_div.attr('style', 'position: relative; clear: both; outline: 0');\n","\n","    function canvas_keyboard_event(event) {\n","        return fig.key_event(event, event['data']);\n","    }\n","\n","    canvas_div.keydown('key_press', canvas_keyboard_event);\n","    canvas_div.keyup('key_release', canvas_keyboard_event);\n","    this.canvas_div = canvas_div\n","    this._canvas_extra_style(canvas_div)\n","    this.root.append(canvas_div);\n","\n","    var canvas = $('<canvas/>');\n","    canvas.addClass('mpl-canvas');\n","    canvas.attr('style', \"left: 0; top: 0; z-index: 0; outline: 0\")\n","\n","    this.canvas = canvas[0];\n","    this.context = canvas[0].getContext(\"2d\");\n","\n","    var backingStore = this.context.backingStorePixelRatio ||\n","\tthis.context.webkitBackingStorePixelRatio ||\n","\tthis.context.mozBackingStorePixelRatio ||\n","\tthis.context.msBackingStorePixelRatio ||\n","\tthis.context.oBackingStorePixelRatio ||\n","\tthis.context.backingStorePixelRatio || 1;\n","\n","    mpl.ratio = (window.devicePixelRatio || 1) / backingStore;\n","\n","    var rubberband = $('<canvas/>');\n","    rubberband.attr('style', \"position: absolute; left: 0; top: 0; z-index: 1;\")\n","\n","    var pass_mouse_events = true;\n","\n","    canvas_div.resizable({\n","        start: function(event, ui) {\n","            pass_mouse_events = false;\n","        },\n","        resize: function(event, ui) {\n","            fig.request_resize(ui.size.width, ui.size.height);\n","        },\n","        stop: function(event, ui) {\n","            pass_mouse_events = true;\n","            fig.request_resize(ui.size.width, ui.size.height);\n","        },\n","    });\n","\n","    function mouse_event_fn(event) {\n","        if (pass_mouse_events)\n","            return fig.mouse_event(event, event['data']);\n","    }\n","\n","    rubberband.mousedown('button_press', mouse_event_fn);\n","    rubberband.mouseup('button_release', mouse_event_fn);\n","    // Throttle sequential mouse events to 1 every 20ms.\n","    rubberband.mousemove('motion_notify', mouse_event_fn);\n","\n","    rubberband.mouseenter('figure_enter', mouse_event_fn);\n","    rubberband.mouseleave('figure_leave', mouse_event_fn);\n","\n","    canvas_div.on(\"wheel\", function (event) {\n","        event = event.originalEvent;\n","        event['data'] = 'scroll'\n","        if (event.deltaY < 0) {\n","            event.step = 1;\n","        } else {\n","            event.step = -1;\n","        }\n","        mouse_event_fn(event);\n","    });\n","\n","    canvas_div.append(canvas);\n","    canvas_div.append(rubberband);\n","\n","    this.rubberband = rubberband;\n","    this.rubberband_canvas = rubberband[0];\n","    this.rubberband_context = rubberband[0].getContext(\"2d\");\n","    this.rubberband_context.strokeStyle = \"#000000\";\n","\n","    this._resize_canvas = function(width, height) {\n","        // Keep the size of the canvas, canvas container, and rubber band\n","        // canvas in synch.\n","        canvas_div.css('width', width)\n","        canvas_div.css('height', height)\n","\n","        canvas.attr('width', width * mpl.ratio);\n","        canvas.attr('height', height * mpl.ratio);\n","        canvas.attr('style', 'width: ' + width + 'px; height: ' + height + 'px;');\n","\n","        rubberband.attr('width', width);\n","        rubberband.attr('height', height);\n","    }\n","\n","    // Set the figure to an initial 600x600px, this will subsequently be updated\n","    // upon first draw.\n","    this._resize_canvas(600, 600);\n","\n","    // Disable right mouse context menu.\n","    $(this.rubberband_canvas).bind(\"contextmenu\",function(e){\n","        return false;\n","    });\n","\n","    function set_focus () {\n","        canvas.focus();\n","        canvas_div.focus();\n","    }\n","\n","    window.setTimeout(set_focus, 100);\n","}\n","\n","mpl.figure.prototype._init_toolbar = function() {\n","    var fig = this;\n","\n","    var nav_element = $('<div/>');\n","    nav_element.attr('style', 'width: 100%');\n","    this.root.append(nav_element);\n","\n","    // Define a callback function for later on.\n","    function toolbar_event(event) {\n","        return fig.toolbar_button_onclick(event['data']);\n","    }\n","    function toolbar_mouse_event(event) {\n","        return fig.toolbar_button_onmouseover(event['data']);\n","    }\n","\n","    for(var toolbar_ind in mpl.toolbar_items) {\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) {\n","            // put a spacer in here.\n","            continue;\n","        }\n","        var button = $('<button/>');\n","        button.addClass('ui-button ui-widget ui-state-default ui-corner-all ' +\n","                        'ui-button-icon-only');\n","        button.attr('role', 'button');\n","        button.attr('aria-disabled', 'false');\n","        button.click(method_name, toolbar_event);\n","        button.mouseover(tooltip, toolbar_mouse_event);\n","\n","        var icon_img = $('<span/>');\n","        icon_img.addClass('ui-button-icon-primary ui-icon');\n","        icon_img.addClass(image);\n","        icon_img.addClass('ui-corner-all');\n","\n","        var tooltip_span = $('<span/>');\n","        tooltip_span.addClass('ui-button-text');\n","        tooltip_span.html(tooltip);\n","\n","        button.append(icon_img);\n","        button.append(tooltip_span);\n","\n","        nav_element.append(button);\n","    }\n","\n","    var fmt_picker_span = $('<span/>');\n","\n","    var fmt_picker = $('<select/>');\n","    fmt_picker.addClass('mpl-toolbar-option ui-widget ui-widget-content');\n","    fmt_picker_span.append(fmt_picker);\n","    nav_element.append(fmt_picker_span);\n","    this.format_dropdown = fmt_picker[0];\n","\n","    for (var ind in mpl.extensions) {\n","        var fmt = mpl.extensions[ind];\n","        var option = $(\n","            '<option/>', {selected: fmt === mpl.default_extension}).html(fmt);\n","        fmt_picker.append(option);\n","    }\n","\n","    // Add hover states to the ui-buttons\n","    $( \".ui-button\" ).hover(\n","        function() { $(this).addClass(\"ui-state-hover\");},\n","        function() { $(this).removeClass(\"ui-state-hover\");}\n","    );\n","\n","    var status_bar = $('<span class=\"mpl-message\"/>');\n","    nav_element.append(status_bar);\n","    this.message = status_bar[0];\n","}\n","\n","mpl.figure.prototype.request_resize = function(x_pixels, y_pixels) {\n","    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n","    // which will in turn request a refresh of the image.\n","    this.send_message('resize', {'width': x_pixels, 'height': y_pixels});\n","}\n","\n","mpl.figure.prototype.send_message = function(type, properties) {\n","    properties['type'] = type;\n","    properties['figure_id'] = this.id;\n","    this.ws.send(JSON.stringify(properties));\n","}\n","\n","mpl.figure.prototype.send_draw_message = function() {\n","    if (!this.waiting) {\n","        this.waiting = true;\n","        this.ws.send(JSON.stringify({type: \"draw\", figure_id: this.id}));\n","    }\n","}\n","\n","\n","mpl.figure.prototype.handle_save = function(fig, msg) {\n","    var format_dropdown = fig.format_dropdown;\n","    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n","    fig.ondownload(fig, format);\n","}\n","\n","\n","mpl.figure.prototype.handle_resize = function(fig, msg) {\n","    var size = msg['size'];\n","    if (size[0] != fig.canvas.width || size[1] != fig.canvas.height) {\n","        fig._resize_canvas(size[0], size[1]);\n","        fig.send_message(\"refresh\", {});\n","    };\n","}\n","\n","mpl.figure.prototype.handle_rubberband = function(fig, msg) {\n","    var x0 = msg['x0'] / mpl.ratio;\n","    var y0 = (fig.canvas.height - msg['y0']) / mpl.ratio;\n","    var x1 = msg['x1'] / mpl.ratio;\n","    var y1 = (fig.canvas.height - msg['y1']) / mpl.ratio;\n","    x0 = Math.floor(x0) + 0.5;\n","    y0 = Math.floor(y0) + 0.5;\n","    x1 = Math.floor(x1) + 0.5;\n","    y1 = Math.floor(y1) + 0.5;\n","    var min_x = Math.min(x0, x1);\n","    var min_y = Math.min(y0, y1);\n","    var width = Math.abs(x1 - x0);\n","    var height = Math.abs(y1 - y0);\n","\n","    fig.rubberband_context.clearRect(\n","        0, 0, fig.canvas.width / mpl.ratio, fig.canvas.height / mpl.ratio);\n","\n","    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n","}\n","\n","mpl.figure.prototype.handle_figure_label = function(fig, msg) {\n","    // Updates the figure title.\n","    fig.header.textContent = msg['label'];\n","}\n","\n","mpl.figure.prototype.handle_cursor = function(fig, msg) {\n","    var cursor = msg['cursor'];\n","    switch(cursor)\n","    {\n","    case 0:\n","        cursor = 'pointer';\n","        break;\n","    case 1:\n","        cursor = 'default';\n","        break;\n","    case 2:\n","        cursor = 'crosshair';\n","        break;\n","    case 3:\n","        cursor = 'move';\n","        break;\n","    }\n","    fig.rubberband_canvas.style.cursor = cursor;\n","}\n","\n","mpl.figure.prototype.handle_message = function(fig, msg) {\n","    fig.message.textContent = msg['message'];\n","}\n","\n","mpl.figure.prototype.handle_draw = function(fig, msg) {\n","    // Request the server to send over a new figure.\n","    fig.send_draw_message();\n","}\n","\n","mpl.figure.prototype.handle_image_mode = function(fig, msg) {\n","    fig.image_mode = msg['mode'];\n","}\n","\n","mpl.figure.prototype.updated_canvas_event = function() {\n","    // Called whenever the canvas gets updated.\n","    this.send_message(\"ack\", {});\n","}\n","\n","// A function to construct a web socket function for onmessage handling.\n","// Called in the figure constructor.\n","mpl.figure.prototype._make_on_message_function = function(fig) {\n","    return function socket_on_message(evt) {\n","        if (evt.data instanceof Blob) {\n","            /* FIXME: We get \"Resource interpreted as Image but\n","             * transferred with MIME type text/plain:\" errors on\n","             * Chrome.  But how to set the MIME type?  It doesn't seem\n","             * to be part of the websocket stream */\n","            evt.data.type = \"image/png\";\n","\n","            /* Free the memory for the previous frames */\n","            if (fig.imageObj.src) {\n","                (window.URL || window.webkitURL).revokeObjectURL(\n","                    fig.imageObj.src);\n","            }\n","\n","            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n","                evt.data);\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        }\n","        else if (typeof evt.data === 'string' && evt.data.slice(0, 21) == \"data:image/png;base64\") {\n","            fig.imageObj.src = evt.data;\n","            fig.updated_canvas_event();\n","            fig.waiting = false;\n","            return;\n","        }\n","\n","        var msg = JSON.parse(evt.data);\n","        var msg_type = msg['type'];\n","\n","        // Call the  \"handle_{type}\" callback, which takes\n","        // the figure and JSON message as its only arguments.\n","        try {\n","            var callback = fig[\"handle_\" + msg_type];\n","        } catch (e) {\n","            console.log(\"No handler for the '\" + msg_type + \"' message type: \", msg);\n","            return;\n","        }\n","\n","        if (callback) {\n","            try {\n","                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n","                callback(fig, msg);\n","            } catch (e) {\n","                console.log(\"Exception inside the 'handler_\" + msg_type + \"' callback:\", e, e.stack, msg);\n","            }\n","        }\n","    };\n","}\n","\n","// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n","mpl.findpos = function(e) {\n","    //this section is from http://www.quirksmode.org/js/events_properties.html\n","    var targ;\n","    if (!e)\n","        e = window.event;\n","    if (e.target)\n","        targ = e.target;\n","    else if (e.srcElement)\n","        targ = e.srcElement;\n","    if (targ.nodeType == 3) // defeat Safari bug\n","        targ = targ.parentNode;\n","\n","    // jQuery normalizes the pageX and pageY\n","    // pageX,Y are the mouse positions relative to the document\n","    // offset() returns the position of the element relative to the document\n","    var x = e.pageX - $(targ).offset().left;\n","    var y = e.pageY - $(targ).offset().top;\n","\n","    return {\"x\": x, \"y\": y};\n","};\n","\n","/*\n"," * return a copy of an object with only non-object keys\n"," * we need this to avoid circular references\n"," * http://stackoverflow.com/a/24161582/3208463\n"," */\n","function simpleKeys (original) {\n","  return Object.keys(original).reduce(function (obj, key) {\n","    if (typeof original[key] !== 'object')\n","        obj[key] = original[key]\n","    return obj;\n","  }, {});\n","}\n","\n","mpl.figure.prototype.mouse_event = function(event, name) {\n","    var canvas_pos = mpl.findpos(event)\n","\n","    if (name === 'button_press')\n","    {\n","        this.canvas.focus();\n","        this.canvas_div.focus();\n","    }\n","\n","    var x = canvas_pos.x * mpl.ratio;\n","    var y = canvas_pos.y * mpl.ratio;\n","\n","    this.send_message(name, {x: x, y: y, button: event.button,\n","                             step: event.step,\n","                             guiEvent: simpleKeys(event)});\n","\n","    /* This prevents the web browser from automatically changing to\n","     * the text insertion cursor when the button is pressed.  We want\n","     * to control all of the cursor setting manually through the\n","     * 'cursor' event from matplotlib */\n","    event.preventDefault();\n","    return false;\n","}\n","\n","mpl.figure.prototype._key_event_extra = function(event, name) {\n","    // Handle any extra behaviour associated with a key event\n","}\n","\n","mpl.figure.prototype.key_event = function(event, name) {\n","\n","    // Prevent repeat events\n","    if (name == 'key_press')\n","    {\n","        if (event.which === this._key)\n","            return;\n","        else\n","            this._key = event.which;\n","    }\n","    if (name == 'key_release')\n","        this._key = null;\n","\n","    var value = '';\n","    if (event.ctrlKey && event.which != 17)\n","        value += \"ctrl+\";\n","    if (event.altKey && event.which != 18)\n","        value += \"alt+\";\n","    if (event.shiftKey && event.which != 16)\n","        value += \"shift+\";\n","\n","    value += 'k';\n","    value += event.which.toString();\n","\n","    this._key_event_extra(event, name);\n","\n","    this.send_message(name, {key: value,\n","                             guiEvent: simpleKeys(event)});\n","    return false;\n","}\n","\n","mpl.figure.prototype.toolbar_button_onclick = function(name) {\n","    if (name == 'download') {\n","        this.handle_save(this, null);\n","    } else {\n","        this.send_message(\"toolbar_button\", {name: name});\n","    }\n","};\n","\n","mpl.figure.prototype.toolbar_button_onmouseover = function(tooltip) {\n","    this.message.textContent = tooltip;\n","};\n","mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Pan axes with left mouse, zoom with right\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n","\n","mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n","\n","mpl.default_extension = \"png\";var comm_websocket_adapter = function(comm) {\n","    // Create a \"websocket\"-like object which calls the given IPython comm\n","    // object with the appropriate methods. Currently this is a non binary\n","    // socket, so there is still some room for performance tuning.\n","    var ws = {};\n","\n","    ws.close = function() {\n","        comm.close()\n","    };\n","    ws.send = function(m) {\n","        //console.log('sending', m);\n","        comm.send(m);\n","    };\n","    // Register the callback with on_msg.\n","    comm.on_msg(function(msg) {\n","        //console.log('receiving', msg['content']['data'], msg);\n","        // Pass the mpl event to the overridden (by mpl) onmessage function.\n","        ws.onmessage(msg['content']['data'])\n","    });\n","    return ws;\n","}\n","\n","mpl.mpl_figure_comm = function(comm, msg) {\n","    // This is the function which gets called when the mpl process\n","    // starts-up an IPython Comm through the \"matplotlib\" channel.\n","\n","    var id = msg.content.data.id;\n","    // Get hold of the div created by the display call when the Comm\n","    // socket was opened in Python.\n","    var element = $(\"#\" + id);\n","    var ws_proxy = comm_websocket_adapter(comm)\n","\n","    function ondownload(figure, format) {\n","        window.open(figure.imageObj.src);\n","    }\n","\n","    var fig = new mpl.figure(id, ws_proxy,\n","                           ondownload,\n","                           element.get(0));\n","\n","    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n","    // web socket which is closed, not our websocket->open comm proxy.\n","    ws_proxy.onopen();\n","\n","    fig.parent_element = element.get(0);\n","    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n","    if (!fig.cell_info) {\n","        console.error(\"Failed to find cell for figure\", id, fig);\n","        return;\n","    }\n","\n","    var output_index = fig.cell_info[2]\n","    var cell = fig.cell_info[0];\n","\n","};\n","\n","mpl.figure.prototype.handle_close = function(fig, msg) {\n","    var width = fig.canvas.width/mpl.ratio\n","    fig.root.unbind('remove')\n","\n","    // Update the output cell to use the data from the current canvas.\n","    fig.push_to_output();\n","    var dataURL = fig.canvas.toDataURL();\n","    // Re-enable the keyboard manager in IPython - without this line, in FF,\n","    // the notebook keyboard shortcuts fail.\n","    IPython.keyboard_manager.enable()\n","    $(fig.parent_element).html('<img src=\"' + dataURL + '\" width=\"' + width + '\">');\n","    fig.close_ws(fig, msg);\n","}\n","\n","mpl.figure.prototype.close_ws = function(fig, msg){\n","    fig.send_message('closing', msg);\n","    // fig.ws.close()\n","}\n","\n","mpl.figure.prototype.push_to_output = function(remove_interactive) {\n","    // Turn the data on the canvas into data in the output cell.\n","    var width = this.canvas.width/mpl.ratio\n","    var dataURL = this.canvas.toDataURL();\n","    this.cell_info[1]['text/html'] = '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n","}\n","\n","mpl.figure.prototype.updated_canvas_event = function() {\n","    // Tell IPython that the notebook contents must change.\n","    IPython.notebook.set_dirty(true);\n","    this.send_message(\"ack\", {});\n","    var fig = this;\n","    // Wait a second, then push the new image to the DOM so\n","    // that it is saved nicely (might be nice to debounce this).\n","    setTimeout(function () { fig.push_to_output() }, 1000);\n","}\n","\n","mpl.figure.prototype._init_toolbar = function() {\n","    var fig = this;\n","\n","    var nav_element = $('<div/>');\n","    nav_element.attr('style', 'width: 100%');\n","    this.root.append(nav_element);\n","\n","    // Define a callback function for later on.\n","    function toolbar_event(event) {\n","        return fig.toolbar_button_onclick(event['data']);\n","    }\n","    function toolbar_mouse_event(event) {\n","        return fig.toolbar_button_onmouseover(event['data']);\n","    }\n","\n","    for(var toolbar_ind in mpl.toolbar_items){\n","        var name = mpl.toolbar_items[toolbar_ind][0];\n","        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n","        var image = mpl.toolbar_items[toolbar_ind][2];\n","        var method_name = mpl.toolbar_items[toolbar_ind][3];\n","\n","        if (!name) { continue; };\n","\n","        var button = $('<button class=\"btn btn-default\" href=\"#\" title=\"' + name + '\"><i class=\"fa ' + image + ' fa-lg\"></i></button>');\n","        button.click(method_name, toolbar_event);\n","        button.mouseover(tooltip, toolbar_mouse_event);\n","        nav_element.append(button);\n","    }\n","\n","    // Add the status bar.\n","    var status_bar = $('<span class=\"mpl-message\" style=\"text-align:right; float: right;\"/>');\n","    nav_element.append(status_bar);\n","    this.message = status_bar[0];\n","\n","    // Add the close button to the window.\n","    var buttongrp = $('<div class=\"btn-group inline pull-right\"></div>');\n","    var button = $('<button class=\"btn btn-mini btn-primary\" href=\"#\" title=\"Stop Interaction\"><i class=\"fa fa-power-off icon-remove icon-large\"></i></button>');\n","    button.click(function (evt) { fig.handle_close(fig, {}); } );\n","    button.mouseover('Stop Interaction', toolbar_mouse_event);\n","    buttongrp.append(button);\n","    var titlebar = this.root.find($('.ui-dialog-titlebar'));\n","    titlebar.prepend(buttongrp);\n","}\n","\n","mpl.figure.prototype._root_extra_style = function(el){\n","    var fig = this\n","    el.on(\"remove\", function(){\n","\tfig.close_ws(fig, {});\n","    });\n","}\n","\n","mpl.figure.prototype._canvas_extra_style = function(el){\n","    // this is important to make the div 'focusable\n","    el.attr('tabindex', 0)\n","    // reach out to IPython and tell the keyboard manager to turn it's self\n","    // off when our div gets focus\n","\n","    // location in version 3\n","    if (IPython.notebook.keyboard_manager) {\n","        IPython.notebook.keyboard_manager.register_events(el);\n","    }\n","    else {\n","        // location in version 2\n","        IPython.keyboard_manager.register_events(el);\n","    }\n","\n","}\n","\n","mpl.figure.prototype._key_event_extra = function(event, name) {\n","    var manager = IPython.notebook.keyboard_manager;\n","    if (!manager)\n","        manager = IPython.keyboard_manager;\n","\n","    // Check for shift+enter\n","    if (event.shiftKey && event.which == 13) {\n","        this.canvas_div.blur();\n","        // select the cell after this one\n","        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n","        IPython.notebook.select(index + 1);\n","    }\n","}\n","\n","mpl.figure.prototype.handle_save = function(fig, msg) {\n","    fig.ondownload(fig, null);\n","}\n","\n","\n","mpl.find_output_cell = function(html_output) {\n","    // Return the cell and output element which can be found *uniquely* in the notebook.\n","    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n","    // IPython event is triggered only after the cells have been serialised, which for\n","    // our purposes (turning an active figure into a static one), is too late.\n","    var cells = IPython.notebook.get_cells();\n","    var ncells = cells.length;\n","    for (var i=0; i<ncells; i++) {\n","        var cell = cells[i];\n","        if (cell.cell_type === 'code'){\n","            for (var j=0; j<cell.output_area.outputs.length; j++) {\n","                var data = cell.output_area.outputs[j];\n","                if (data.data) {\n","                    // IPython >= 3 moved mimebundle to data attribute of output\n","                    data = data.data;\n","                }\n","                if (data['text/html'] == html_output) {\n","                    return [cell, data, j];\n","                }\n","            }\n","        }\n","    }\n","}\n","\n","// Register the function which deals with the matplotlib target/channel.\n","// The kernel may be null if the page has been refreshed.\n","if (IPython.notebook.kernel != null) {\n","    IPython.notebook.kernel.comm_manager.register_target('matplotlib', mpl.mpl_figure_comm);\n","}\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div id='d138f910-1f18-4c00-9838-02a44f471955'></div>"]},"metadata":{}}],"source":["visualize(img, activation_map, pred)  # Visualize CAM"]},{"cell_type":"markdown","id":"rapid-vintage","metadata":{"id":"rapid-vintage"},"source":["The heatmap focuses on the area which shows signs of pneumonia, isn't that cool?"]},{"cell_type":"markdown","id":"existing-neighborhood","metadata":{"id":"existing-neighborhood"},"source":["Awesome, you made it! <br />\n","You can try out this method for all neural networks with one restriction: The classic CAM algorithm requrires a specific network architecture:<br />\n","Last Convolution $\\rightarrow$ Global Average Pool $\\rightarrow$ 1 FC Layer.\n","\n","For other network architectures, you either need to adapt the architecture, followed by retraining our use different methods such as GradCAM or ScoreCAM (https://arxiv.org/abs/1610.02391), (https://arxiv.org/abs/1910.01279)"]}],"metadata":{"kernelspec":{"display_name":"Udemy","language":"python","name":"udemy"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"03-Interpretability.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}